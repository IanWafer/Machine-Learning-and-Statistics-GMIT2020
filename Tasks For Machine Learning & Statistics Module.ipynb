{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 SQRT2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "We have been tasked with writing a  Python function that calculates the square root of 2 and prints it to the screen to 100 decimal places.\n",
    "\n",
    "The square root of a number is a specified quantity given when multiplied by itself. In mathematics it is generally represented  by the radical sign or radix √. It must be a positive number though in mathematics square roots of negative numbers are sometimes used and  are  known as imaginary numbers. The square root of 2 is known as an irrtaional number as it cannot be expressed as a fraction and has a never ending number of decimal numbers. \n",
    "\n",
    "## History\n",
    "\n",
    "The Pythagorean philosophers had a great focus on numbers and geometry and and guarded it as secret knowledge believeing they wwere gaining insight into  the make up of the universe itself. They believed all numbers were rational and could be expressed as a ratio of integer values. \n",
    "\n",
    "It is believed  that their belief in this concept was so strong that when when Hippasus discovered irrational numbers he was thrown from a boat and left to drown though this  event is seen as more of a myth than concrete factual truth due to conflicting reports from Iamblichus.<sup>[1.1]</sup>\n",
    "\n",
    "## Limitations\n",
    "\n",
    "Python has some built in limitations on how it runs its calculations. This is due to the fact that Python works on a base 2 system which cannot represent all decimal fractions exactly. This results in decimal floating-point numbers being approximated by the binary number stored. Floats are generally calculated using the first 53 bits beginning with the significant bit with a denominator as a power of 2.\n",
    "\n",
    "This is not a unique feature of a binary system. All base systems have limitations on their accuracies. If we consider our well understood base 10 system the fraction 1/3 can never be brought to it's full decimal representation. It can be shown to be 0.3, 0.33,  0.333 etc. Each additionnal decimal point brings us closer to the true value but we never reach the final value.<sup>[1.2]</sup>\n",
    "\n",
    "## Computational Methods\n",
    "\n",
    "There are several computational methods of calcluting the squareroot of a number. We shall be delving into the below methods-\n",
    "* Babylonian Method\n",
    "* Newton-Raphson Method\n",
    "\n",
    "## Babylonian Method\n",
    "\n",
    "This was one of the first known alogorithms for computing the square root of a number. It is believed to have been discovered in about 1,500BC by the babylonians and is also known as the Heron's Method. It works by guessing an initial over estimate number x<sub>0</sub> to the square root of a non negative real number S. Then S/x<sub>0</sub> will be an under estimate providing a better approximation of the real value by an average of the 2 numbers using the below formula. This is a special case of the Newton formula as you can see in the Newton-Raphson method below. <sup>[1.3][1.4]</sup>\n",
    "\n",
    "<img align=\"left\" src=\"images/Task_1/babylonian_average.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 value is 1.5\n",
      "Iteration 2 value is 1.4166666666666665\n",
      "Iteration 3 value is 1.4142156862745097\n",
      "Iteration 4 value is 1.4142135623746899\n",
      "Iteration 5 value is 1.414213562373095\n",
      "Iteration 6 value is 1.414213562373095\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.4142135623730949234300169337075203657150268554687500000000000000000000000000000000000000000000000000'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def BABYLONIAN(n): # https://www.w3resource.com/python-exercises/math/python-math-exercise-18.php\n",
    "    \n",
    "    if(n == 0):\n",
    "        return 0;\n",
    "\n",
    "    g = n/2.0; #First guess \n",
    "    g2 = g + 1; \n",
    "    Iteration = 1;\n",
    "    while(g != g2):      \n",
    "        num = n/ g;\n",
    "        g2 = g;\n",
    "        g = (g + num)/2;\n",
    "        print(\"Iteration\", Iteration, \"value is\",g)\n",
    "        Iteration = Iteration + 1\n",
    "\n",
    "    return (\"%.100f\" %g)\n",
    "\n",
    "BABYLONIAN(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this does give us an approximate value of the square root off 2 this still does not solve thhe issue at hand to find thhe square root of 2 due to thhe limitations of pythong mentioned above. \n",
    "\n",
    "From the government organsiation Nasa we can see that the square root of 2 to 100 decimal place is 1.4142135623730950488016887242096980785696718753769480731766797379907324784621070388503875343276415727<sup>[1.5]</sup>\n",
    "\n",
    "To overcome this we will be using thee Newton-Raphson method shown below with the new greatly increased to remove the float limitations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton-Raphson Method\n",
    "\n",
    "The Newton-Raphson method is an iterative method where each succesive iteration brings the result closer to the true value of the answer after finding a suitable starting value. The closer the initially selected value is to the answer the less itteratons  are required to produce the answer. It uses the idea that a continuous and differentiable function can be approximated by a straight line tangent to it.\n",
    "\n",
    "If x = √a for a > 0 then we need to solve for x<sup>2</sup> = a. To start Newtons method of solving this problem we need to have some guess for the value of x where x<sub>1</sub>>0. \n",
    "\n",
    "We can then begin with the first iteration with a single variable function f and an initial guess x<sub>0</sub> \n",
    "\n",
    "<img align=\"left\" src=\"images/Task_1/first_iteration.png\"><br><br>\n",
    "\n",
    "Succesive iterations can then be represented by the formula-\n",
    "\n",
    "<img align=\"left\" src=\"images/Task_1/newton_function_formula.png\"><br><br>\n",
    "\n",
    "This can be further developed into the equation below to calculate our new iterative values-\n",
    "\n",
    "<img align=\"left\" src=\"images/Task_1/newton_formula.PNG\"><br><br>\n",
    "\n",
    "Each iteration calculated will bring us closer to the true value required. Belowe we can see the code required to calculate the value of the squre root we require to the correct decimal  precision. To do this we have greatly incresed the initial value to overcome the limitations of pythons floats and then at the end cut it back down to give us the value we require.<sup>[1.6][1.7][1.8][1.9]</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4142135623730950488016887242096980785696718753769480731766797379907324784621070388503875343276415727\n"
     ]
    }
   ],
   "source": [
    "def NEWTON(n): # https://stackoverflow.com/questions/64278117/is-there-a-way-to-create-more-decimal-points-on-python-without-importing-a-libra\n",
    "    x = n * 10 ** 200 # Value increased to get required precision due to Python limitations\n",
    "    r = x\n",
    "\n",
    "    if(x == 0):\n",
    "        return 0;\n",
    "    \n",
    "    def test_diffs(x, r):\n",
    "        d0 = abs(x - r**2)\n",
    "        dm = abs(x - (r-1)**2)\n",
    "        dp = abs(x - (r+1)**2)\n",
    "        minimised = d0 <= dm and d0 <= dp\n",
    "        below_min = dp < dm\n",
    "        return minimised, below_min\n",
    "\n",
    "    while True:\n",
    "        oldr = r\n",
    "        r = (r + x // r) // 2\n",
    "\n",
    "        minimised, below_min = test_diffs(x, r)\n",
    "        if minimised:\n",
    "            break\n",
    "\n",
    "        if r == oldr:\n",
    "            if below_min:\n",
    "                r += 1\n",
    "            else:\n",
    "                r -= 1\n",
    "            minimised, _ = test_diffs(x, r)\n",
    "            if minimised:\n",
    "                break\n",
    "\n",
    "    print(f'{r // 10**100}.{r % 10**100:0100d}')\n",
    "\n",
    "NEWTON(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Due too the limitations of pythons ability to count floats the number had to be greatly increased to determine an accurate value of the required square root and then divided  back down again before displaying the final answer. The base 2 system should always be a consideration when runing any kind of calculations especially in an environment where pin point accuracy is essential for effective implementation of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources-\n",
    "\n",
    "[1.1] wikipedia.org, \"Hippasus\", [online], https://en.wikipedia.org/wiki/Hippasus#Irrational_numbers\n",
    "\n",
    "[1.2] python.org, \"floating Point\", [online], https://docs.python.org/3/tutorial/floatingpoint.html\n",
    "\n",
    "[1.3] wikipedia.org, \"Methods of computing square roots\", [online], https://en.wikipedia.org/wiki/Methods_of_computing_square_roots#Babylonian_method\n",
    "\n",
    "[1.4] w3resource, \"python exercises\"https://www.w3resource.com/python-exercises/math/python-math-exercise-18.php\n",
    "\n",
    "[1.5] apod.nasa.gov, \"sqrt2\", [online], https://apod.nasa.gov/htmltest/gifcity/sqrt2.1mil\n",
    "\n",
    "[1.6] wikipedia.org, \"Newtons Method\", [online], https://en.wikipedia.org/wiki/Newton%27s_method\n",
    "\n",
    "[1.7] brilliant, \"newton-sqrt\", [online], https://brilliant.org/wiki/newton-raphson-method/\n",
    "\n",
    "[1.8] math.mit.edu, \"newton-sqrt\", [online], https://math.mit.edu/~stevenj/18.335/newton-sqrt.pdf\n",
    "\n",
    "[1.9] cosmomagazine.com, \"the-square-root-of-2\", [online], https://cosmosmagazine.com/mathematics/the-square-root-of-2/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 Chi-squared Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Task 2 has specified that we need to use the scipy.stats to verify the chi-squared value of the of the array given as shown below and calculate its  associated <i>p</i> value. We also need to include a short note with references justifying the analysis  in a markdown cell.\n",
    "\n",
    "![chi_squared_array](images/Task_2/chi_squared_array.png)\n",
    "\n",
    "## History\n",
    "\n",
    "The chi squared test can also be written as the <i>χ<sup>2</sup></i>. It is a statistical hypothesis test for checking if two categorical variables are related in some population by checking if there is a statistically significant difference between the expected outcomes and the observed frequencies in the categories of a contingency table. It was published in a paper by Karl Pearson in 1900 and is considered a foundation of modern statistics. <sup>[2.1]</sup><sup>[2.2]</sup>\n",
    "\n",
    "## Use Case\n",
    "\n",
    "The purpose of the test is to evaluate a null hypotesis showing the frequency distribution of events in a certain sample  size are in line with the theoretical predicted outcome.\n",
    "\n",
    "The null hypothesis in statistics refers to the default assumption that there are no differences between two measured phenomena oro that the sampeles are taken from the same source. \n",
    "\n",
    "The chi-squared test is used regularly in cryptographic problems and bioinformatics in comparing properties of genes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contingency Table\n",
      "                A    B    C    D  Total\n",
      "White Collar   90   60  104   95    349\n",
      "Blue Collar    30   50   51   20    151\n",
      "No Collar      30   40   45   35    150\n",
      "Total         150  150  200  150    650\n",
      "\n",
      "Stat: 24.571202858582602\n",
      "p value: 0.016990737760739776\n",
      "Degrees of freedom: 12 \n",
      "\n",
      "Expected Table\n",
      "[[ 80.53846154  80.53846154 107.38461538  80.53846154 349.        ]\n",
      " [ 34.84615385  34.84615385  46.46153846  34.84615385 151.        ]\n",
      " [ 34.61538462  34.61538462  46.15384615  34.61538462 150.        ]\n",
      " [150.         150.         200.         150.         650.        ]]\n",
      "\n",
      "Interpret test-statistic\n",
      "probability=0.95, critical=21.03, stat=24.6\n",
      "Dependent (reject H0)\n",
      "\n",
      "Interpret p-value\n",
      "significance=0.050, p=0.017\n",
      "Dependent (reject H0)\n"
     ]
    }
   ],
   "source": [
    "# import required packages to run the test\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "\n",
    "# create the array required representing the table date shown above\n",
    "table = pd.DataFrame([[90,60,104,95,349],[30,50,51,20,151],[30,40,45,35,150],[150,150,200,150,650]], index=[\"White Collar\",\"Blue Collar\",\"No Collar\",\"Total\"], columns=[\"A\",\"B\",\"C\",\"D\",\"Total\"])\n",
    "print(\"Contingency Table\")\n",
    "print(table)\n",
    "\n",
    "# run the chi test on the created contingency table above and assign variables\n",
    "stat,p,dof,expected = ss.chi2_contingency(table) \n",
    "print(\"\\nStat:\",stat) #  Sum of the totals chi square points. Chi square points =(Observed  Expected)^2/Expected \n",
    "print(\"p value:\",p) # a measure of the probability that an observed difference could have occurred just by random chance#\n",
    "print('Degrees of freedom: %d \\n' % dof) # degrees of freedom (total columns -1)*(total rows -1)\n",
    "\n",
    "print(\"Expected Table\")\n",
    "print(expected) # expected return array\n",
    "\n",
    "print(\"\\nInterpret test-statistic\")\n",
    "prob = 0.95\n",
    "critical = ss.chi2.ppf(prob, dof)\n",
    "\n",
    "print('probability=%.2f, critical=%.2f, stat=%.1f' % (prob, critical, stat))\n",
    "\n",
    "if abs(stat) >= critical:\n",
    "    print('Dependent (reject H0)')\n",
    "else:\n",
    "    print('Independent (fail to reject H0)')\n",
    "    \n",
    "print(\"\\nInterpret p-value\")\n",
    "alpha = 1.0 - prob\n",
    "print('significance=%.3f, p=%.3f' % (alpha, p))\n",
    "\n",
    "if p <= alpha:\n",
    "    print('Dependent (reject H0)')\n",
    "else:\n",
    "    print('Independent (fail to reject H0)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources-\n",
    "\n",
    "[2.1] wikipedia.org, \"Chi-squared_test\", [online], https://en.wikipedia.org/wiki/Chi-squared_test\n",
    "\n",
    "[2.2] wikipedia.org, \"Pearson's chi-squared test\", [online], https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test\n",
    "\n",
    "https://machinelearningmastery.com/chi-squared-test-for-machine-learning/\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2_contingency.html\n",
    "\n",
    "https://stattrek.com/chi-square-test/goodness-of-fit.aspx#:~:text=The%20P%2Dvalue%20is%20the%20probability%20that%20a%20chi%2Dsquare,2%20%3E%2019.58)%20%3D%200.0001."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 Standard Deviation of an Array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducution\n",
    "\n",
    "For task 3 we have been asked to research thee excel functions STDEV.P and STDEV.S noting the differences between these two similar functions.  We are then tasked with using the numpy package to perform a simiulation demonstrating that STDEV.S caculation is a better estimate for the standard deviation of the population when performed on a sample. Part of the task is also figuring out the various terminology used.\n",
    "\n",
    "## Research\n",
    "\n",
    "The standard deviation of an array of numbers x is calculated using numpy with the below notation-\n",
    "\n",
    "np.sqrt(np.sum((x-np.mean(x)^2)/len(x)).\n",
    "\n",
    "To use this an excel we would use the functions STDEV.P (standard deviation population) and STDEV.S (Standard deviation sample). STDEV.P is to be used when the data represents the entire population.  If just a sample of the population is given then STDEV.S is used. \n",
    "\n",
    "The difference between these two feature is in the division.  In STDEV.P the squared  deviation is divided by the total number of arguments represented by x in the above numpy formula. In STDEV.S the squared deviation is divided by the total number of samples -1. <sup>[3.1]</sup>\n",
    "\n",
    "Before we move forward we should define a few of the functions we're discussing. \n",
    "\n",
    "##  Definitions\n",
    "\n",
    "<b>Population-</b> The entire pool which a sample is taken and known as a parameter. This can vary in size depending on the desired information the user is looking for. It can be defined as an group of subjects sharing a common feature. It is generally represented by the upper case N in statistics. <sup>[3.2]</sup>\n",
    "\n",
    "<b>Sample-</b> A sample refers to a subset of the population and is known as a statistic. Samples are generally used to have a more manageable version of the larger population and carry the same characteristic features of the population. It is important to ensure that the sample taken covers the range of features required. If a sample is used in place of population it is generally represented by the lower case n. <sup>[3.3]</sup>\n",
    "\n",
    "<b>Mean-</b> The sum of the datapoints divided by the total quantity of datapoints. This is represented slightly differently between population and samples but is ultimately the same equation as shown below.\n",
    "- <i>Population-</i>\n",
    "\\begin{align*}\n",
    "\\mu= \n",
    "\\frac\n",
    "    {\\sum_{i=1}^N x_{i}}\n",
    "    {N}\n",
    "\\end{align*}\n",
    "\n",
    "- <i>Sample-</i>\n",
    "\\begin{align*}\n",
    "\\overline{x}= \n",
    "\\frac\n",
    "    {\\sum_{i=1}^n x_{i}}\n",
    "    {n}\n",
    "\\end{align*}\n",
    "\n",
    "<b>Variability-</b> This refers to how spread out the datapoints of a series are from their average value. The higher the number the larger the gaps  between a sequence of numbers. There is a diffference between the population and sample formulas used and also within the sample formulas themselves there are two formulas to be considered known as the biased and unbiased  formulas. Bias will  be explained shortly.\n",
    "\n",
    "- <i>Population-</i>\n",
    "\\begin{align*}\n",
    "\\sigma^2 = \n",
    "\\frac\n",
    "        {\\sum_{i=1}^N (x_{i} - \\mu)^2}\n",
    "        {N}\n",
    "\\end{align*}\n",
    "\n",
    "- <i>Sample-</i>\n",
    "\n",
    "<u>Biased Estimate</u>\n",
    "\\begin{align*}\n",
    "S_{n}^2 = \n",
    "\\frac\n",
    "        {\\sum_{i=1}^n (x_{i} - \\overline{x})^2}\n",
    "        {n}\n",
    "\\end{align*}\n",
    "\n",
    "<u>Unbiased Estimate</u>\n",
    "\\begin{align*}\n",
    "S_{n-1}^2 = \n",
    "\\frac\n",
    "        {\\sum_{i=1}^n (x_{i} - \\overline{x})^2}\n",
    "        {n-1}\n",
    "\\end{align*}\n",
    "\n",
    "<b>Standard Deviation-</b> This is the square root of the variance shown above and brings the units back to the non squared units. It is a measure of dispersion of the values of a series from their mean. Just like in variability there are seperate equations for the population and sample calculations shown below.\n",
    "\n",
    "- <i>Population-</i> The population standard deviation is a fixed value calculated for all datapoints of the population and is represented by the equation below-\n",
    "\\begin{align*}\n",
    "\\sigma = \n",
    "\\sqrt[]{\n",
    "    \\frac\n",
    "        {\\sum_{i=1}^N (x_{i} - \\mu)^2}\n",
    "        {N}\n",
    "}\n",
    "\\end{align*}\n",
    "\n",
    "- <i>Sample-</i> A sample  is a subset of the population known as a statistic. The equation for this is shown below. This is slightly different than the population standard deviation in that it's divided  by n-1 instead of n.  This is known as the unbiased estimate of the population and the reasons will be explained shortly-\n",
    "\n",
    "\\begin{align*}\n",
    "\\sigma = \n",
    "\\sqrt[]{\n",
    "    \\frac\n",
    "        {\\sum_{i=1}^n (x_{i} - \\overline{x})^2}\n",
    "        {n-1}\n",
    "}\n",
    "\\end{align*}\n",
    "\n",
    "<b>Deviation-</b> The difference between an observed vlue and some other value. Generally the other value is the mean as is the case n the standard deviation formula above. It is represented by the equation-\n",
    "\\begin{align*}\n",
    "(x_{i} - \\overline{x})\n",
    "\\end{align*}\n",
    "\n",
    "<b>Bias</b>\n",
    "\n",
    "As we can see from the equations above the standard deviation for the sample has a smaller division than the standard deviation formula for the population. Because the sample is only a subset of the population the standard deviation calculated can have great variability. This results in the sample standard deviation being a larger value than the population standard deviation if using the popluation formula of N in place of the samples required n-1. This is know as a biased estimate of the population. \n",
    "\n",
    "To avoid this issue we use the unbiased form of the equation for a sample. This is a way of accounting for the larger variabaility within the sample compared to the populaton due  to the fact that it's only a set amount of the population being used as datapoints. As the n value grows toward infinity the n-1 values becomes less and less relevant in magnitude.<sup>[3.5]</sup>\n",
    "\n",
    "Below is an example of the standard deviation calculation of a population of 1,000 datapoints of  a number between 1 and 100. The standard deviation of  the total population is calculated and then a random sample of 50  is taken from the generated population and both the biased and unbiased standard deviation is calculated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The standard deviation of the population is 29.32237371018929\n",
      "\n",
      "Using STDEV.P the average reading of a random sample of 50  taken from a population of 1,000 done over 100 iterations returns as 28.768235448477576. This is a difference from the actual population standard deviation of 0.5541382617117137\n",
      "\n",
      "Using STDEV.S the average reading of a random sample of 50  taken from a population of 1,000 done over 100 iterations returns as 29.243781473012998. This is a difference from the actual population standard deviation of 0.07859223717629149\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import statistics as stat\n",
    "\n",
    "def stdev_p(pop):\n",
    "    avg_STDDEV_P = []\n",
    "    for i in range(50):\n",
    "        x = np.random.randint(950)\n",
    "        reading = np.std(pop[x:(x+50)])\n",
    "        avg_STDDEV_P.append(reading)\n",
    "    return  stat.mean(avg_STDDEV_P)\n",
    "\n",
    "def stdev_s(pop):\n",
    "    avg_STDDEV_S = []\n",
    "    for i in range(50):\n",
    "        x = np.random.randint(950)\n",
    "        reading = np.std(pop[x:(x+50)], ddof = 1)\n",
    "        avg_STDDEV_S.append(reading)\n",
    "    return  stat.mean(avg_STDDEV_S)\n",
    "    \n",
    "pop = np.random.randint(100, size=1000)\n",
    "\n",
    "pop_STDEV_P = np.std(pop)\n",
    "print(f\"The standard deviation of the population is {pop_STDEV_P}\")\n",
    "\n",
    "STDEV_P = stdev_p(pop)\n",
    "STDEV_S = stdev_s(pop)\n",
    "print(f\"\\nUsing STDEV.P the average reading of a random sample of 50  taken from a population of 1,000 done over 100 iterations returns as {STDEV_P}. This is a difference from the actual population standard deviation of {abs(STDEV_P - pop_STDEV_P)}\")\n",
    "print(f\"\\nUsing STDEV.S the average reading of a random sample of 50  taken from a population of 1,000 done over 100 iterations returns as {STDEV_S}. This is a difference from the actual population standard deviation of {abs(STDEV_S - pop_STDEV_P)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "To achieve  a fair representation of the biased and unbiased calculations the formulas were iterated through 50 times each and an average value taken. This is to minimise statistical  annomalies and give  a better representation of their  true calculated values. \n",
    "\n",
    "From the returned values of the above calculations we can see that while both the biased and unbiased standard deviations are a close approximation of the true standrard deviation of the population, the unbiased calculation is a better estimate for the true  value. \n",
    "\n",
    "As discussed above this is due to thhe greater variability created by taking samples from a population. This leads to an over estimation of the standard deviation which must be accounted for.  The n-1 is a method of accounting for  this over estimaton.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n",
    "\n",
    "[3.1] https://www.exceltip.com/statistical-formulas/how-to-use-excel-stdev-p-function.html#:~:text=Let's%20Explore.-,The%20STDEV.,sample%20of%20the%20entire%20population.\n",
    "\n",
    "[3.2] https://www.investopedia.com/terms/p/population.asp\n",
    "\n",
    "[3.3] https://www.investopedia.com/terms/s/sample.asp\n",
    "\n",
    "[3.4] https://www.khanacademy.org/math/ap-statistics/summarizing-quantitative-data-ap/more-standard-deviation/v/review-and-intuition-why-we-divide-by-n-1-for-the-unbiased-sample-variance\n",
    "\n",
    "[3.5] https://www.khanacademy.org/math/statistics-probability/summarizing-quantitative-data/variance-standard-deviation-sample/a/population-and-sample-standard-deviation-review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4 k-means on the Iris data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the final task we were asked to use scikit-learn to apply k-means clustering to Fisher's Iris data set. We have to explain how the code works and how accurate it might be and explain how the model could be used to make predictions of Iris species.\n",
    "\n",
    "## Research\n",
    "Clustering is an exploratory data analysis technique. It's used to get an idea of the structure of the data and is used in identifying homogenous subgroups within overall data sets. Clusters are as similar as possible according to a similarity measure.<sup>[4.1].</sup>\n",
    "\n",
    "The K-means algorithm seperates samples into a number of groups of equal variance, minimising a criterion known as the inertia or within-cluster sum-of-squares by t he use of centroids and datapoints distance from these points. The number of groups it is to be seperated into is specified by the use. The algorithm is known to scale well to large scale deployment and has been used across mulitple fields and applications. The K-means formula is shown below-<sup>[4.2]</sup>\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{i=0}^n \\min_{\\mu_{j} \\in C} (||x_{i} - \\mu_{j} ||^2)\n",
    "\\end{align*}\n",
    "\n",
    "From the scikit package we get can use the K-means formula with the following command-\n",
    "\n",
    "class sklearn.cluster.KMeans(n_clusters=8, *, init='k-means++', n_init=10, max_iter=300, tol=0.0001, precompute_distances='deprecated', verbose=0, random_state=None, copy_x=True, n_jobs='deprecated', algorithm='auto')\n",
    "\n",
    "For an in depth look at each individual point in this formula please see the link in the source. <sup>[4.3]</sup>\n",
    "\n",
    "First we will be looking at the optimal amount of clusters to input to the ormula for  the dataset. ne method of doing this is know a the elbow method. This runs K-means clustering on the dataset for a range of values of k and for each value calculating the sum of the squared distances. This is then used to plot a chart and the major bend (elbow) on the chart is the number of clusters to use. The goal of this method is to choose a small value of k that still has a low sum of squared distances. The elbow in this case would represent the point of diminishing returns. It should be noted that in cases where the data is very slutered together this method may not be suitable and an alternative amy be required as this would not present a clear elbow on the chart. Alternative methods to use in these cases would be average silhouette method or the gap statistic method. <sup>[4.4]</sup> <sup>[4.5]</sup>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41.13817202297779, 12.143688281579719, 6.9981140048267605, 5.5328310030819, 4.571923613060554, 3.9150575076663006, 3.467963005704488, 3.1562360002952126, 2.819634660252334, 2.5813089377525973, 2.2806931653714244, 2.1308992549753745, 2.0175015190984897, 1.9407640969065174]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#https://blog.cambridgespark.com/how-to-determine-the-optimal-number-of-clusters-for-k-means-clustering-14f27070048f\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv(\"iris.csv\")\n",
    "data.head()\n",
    "\n",
    "# Drop species name column as this is not required for the eelbow calculation\n",
    "del data[\"species\"]\n",
    "\n",
    "mms = MinMaxScaler()\n",
    "mms.fit(data)\n",
    "data_transformed = mms.transform(data)\n",
    "\n",
    "Sum_of_squared_distances = []\n",
    "K = range(1,15)\n",
    "for k in K:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km = km.fit(data_transformed)\n",
    "    Sum_of_squared_distances.append(km.inertia_)\n",
    "    \n",
    "print(Sum_of_squared_distances)\n",
    "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum_of_squared_distances')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "the init parameter for the k-means should be 'k-means++' or 'random' or an ndarray, '21' (type '<class 'str'>') was passed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-13136ade209e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-04\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m )\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0my_km\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# plot the 3 clusters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py\u001b[0m in \u001b[0;36mfit_predict\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1103\u001b[0m             \u001b[0mIndex\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcluster\u001b[0m \u001b[0meach\u001b[0m \u001b[0msample\u001b[0m \u001b[0mbelongs\u001b[0m \u001b[0mto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \"\"\"\n\u001b[1;32m-> 1105\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1107\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mseed\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mseeds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m             \u001b[1;31m# run a k-means once\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1051\u001b[1;33m             labels, inertia, centers, n_iter_ = kmeans_single(\n\u001b[0m\u001b[0;32m   1052\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m                 \u001b[0minit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py\u001b[0m in \u001b[0;36m_kmeans_single_elkan\u001b[1;34m(X, sample_weight, n_clusters, max_iter, init, verbose, x_squared_norms, random_state, tol, n_threads)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m     \u001b[1;31m# init\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m     centers = _init_centroids(X, n_clusters, init, random_state=random_state,\n\u001b[0m\u001b[0;32m    406\u001b[0m                               x_squared_norms=x_squared_norms)\n\u001b[0;32m    407\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py\u001b[0m in \u001b[0;36m_init_centroids\u001b[1;34m(X, n_clusters, init, random_state, x_squared_norms, init_size)\u001b[0m\n\u001b[0;32m    732\u001b[0m         \u001b[0mcenters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcenters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    733\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 734\u001b[1;33m         raise ValueError(\"the init parameter for the k-means should \"\n\u001b[0m\u001b[0;32m    735\u001b[0m                          \u001b[1;34m\"be 'k-means++' or 'random' or an ndarray, \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    736\u001b[0m                          \"'%s' (type '%s') was passed.\" % (init, type(init)))\n",
      "\u001b[1;31mValueError\u001b[0m: the init parameter for the k-means should be 'k-means++' or 'random' or an ndarray, '21' (type '<class 'str'>') was passed."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n",
    "\n",
    "[4.1] https://towardsdatascience.com/k-means-clustering-algorithm-applications-evaluation-methods-and-drawbacks-aa03e644b48a\n",
    "\n",
    "[4.2] https://scikit-learn.org/stable/modules/clustering.html#k-means\n",
    "\n",
    "[4.3] https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans\n",
    "\n",
    "[4.4] https://www.scikit-yb.org/en/latest/api/cluster/elbow.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
